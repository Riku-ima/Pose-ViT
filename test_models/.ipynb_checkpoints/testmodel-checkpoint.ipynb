{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0440d65",
   "metadata": {},
   "source": [
    "## Timesformer + AcT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccaec547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from utils import Dataset\n",
    "from utils import utils\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e50c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model in [Times_AcT , AcT , Densenet]\n",
    "model_name='TestModel'\n",
    "\n",
    "args=EasyDict({\n",
    "    'base_model': model_name,\n",
    "    'pretrained':True,\n",
    "    'lr':0.01,\n",
    "    'start_epoch':1,\n",
    "    'num_epochs':25,\n",
    "    'continue_epoch':False,\n",
    "    \n",
    "    #Dataset parms\n",
    "    'num_classes':10,\n",
    "    'batch_size':8,\n",
    "    #Path params\n",
    "    'model_path':'./models/weights/'+model_name+'/',\n",
    "    'history_path':'./history/'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01799f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitembLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels:int=3,\n",
    "                 emb_dim:int=128,\n",
    "                 image_size=(16,176,128),\n",
    "                 patch_t_size:int=4,\n",
    "                 kernel_size=(2,8,8)):\n",
    "        \"\"\"\n",
    "        stride : (t,h,w)各次元のstride量\n",
    "        \"\"\"\n",
    "        super(VitembLayer,self).__init__()\n",
    "        self.in_channels=in_channels\n",
    "        self.emb_dim=emb_dim\n",
    "        self.patch_size=kernel_size[1]\n",
    "        self.patch_t_size=kernel_size[0]\n",
    "        #各frameをパッチに , 1つのパッチは16*16にする\n",
    "        self.num_patch_h=int(image_size[1]/self.patch_size)\n",
    "        self.num_patch_w=int(image_size[2]/self.patch_size)\n",
    "        self.num_patch_t=int(image_size[0]/self.patch_t_size)\n",
    "        self.image_size=image_size\n",
    "        self.num_patch=self.num_patch_h*self.num_patch_w*self.num_patch_t\n",
    "        self.kernel_size=kernel_size\n",
    "        \n",
    "        self.patch_emb_layer=nn.Conv3d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.emb_dim,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.kernel_size)\n",
    "        \n",
    "        self.cls_token=nn.Parameter(torch.randn(1,1,emb_dim))\n",
    "        self.pos_emb=nn.Parameter(\n",
    "            torch.randn(1,self.num_patch+1,emb_dim))\n",
    "    \n",
    "    def forward(self,x:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        in : x (B,C,T,H,W) \n",
    "        return:\n",
    "                z_0 : 各tubelet (B,N,D)\n",
    "        \"\"\"\n",
    "        \n",
    "        #x -> (B*N,T,P)\n",
    "        #if x.dim()==4:\n",
    "        #    x=x.reshape(-1,self.num_patch,self.input_dim)\n",
    "        b,c,t,h,w=x.shape\n",
    "        z_0=self.patch_emb_layer(x)\n",
    "        z_0=z_0.flatten(2).transpose(1,2)\n",
    "        cls_token=self.cls_token.repeat(repeats=(x.size(0),1,1))\n",
    "        z_0=torch.cat([cls_token,z_0],dim=1)\n",
    "        # Add positional emb\n",
    "        z_0=z_0+self.pos_emb\n",
    "        return z_0\n",
    "    \n",
    "\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self,emb_dim:int=128,num_heads:int=2,dropout:float=0.,with_qkv=True):\n",
    "        \"\"\"\n",
    "        with_qkv :\n",
    "        -> True なら、MHSAのために、3倍のdimに入力をq,k,vに埋め込む\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MHSA,self).__init__()\n",
    "        self.num_heads=num_heads\n",
    "        self.emb_dim=emb_dim\n",
    "        self.head_dim=emb_dim//self.num_heads\n",
    "        self.sqrt_d=emb_dim**0.5\n",
    "        self.with_qkv=with_qkv\n",
    "        \n",
    "        #埋め込み\n",
    "        if with_qkv:\n",
    "            self.w_qkv=nn.Linear(self.emb_dim,self.emb_dim*3,bias=False)\n",
    "            self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.w_o=nn.Sequential(\n",
    "            nn.Linear(emb_dim,emb_dim),\n",
    "            nn.Dropout(dropout))\n",
    "        \n",
    "    def forward(self,z:torch.Tensor)->torch.Tensor:\n",
    "        B,N,D=z.shape\n",
    "        if self.with_qkv:\n",
    "            #->(B,N,3,num_heads,head_dim)\n",
    "        \n",
    "            z=self.w_qkv(z).reshape(B,N,3,self.num_heads,self.head_dim)\n",
    "            # ->(3,B,num_heads,N,head_dim)\n",
    "            z=z.permute(2,0,3,1,4)\n",
    "            q,k,v=z[0],z[1],z[2]\n",
    "        #埋め込みが不要なら、ただqkv複製\n",
    "        else:\n",
    "            z=z.reshape(B,N,self.num_heads,self.head_dim)\n",
    "            #->(B.num_heads ,N ,head_dim )\n",
    "            z=z.permute(0,2,1,3)\n",
    "            q,k,v=z,z,z\n",
    "        \n",
    "        attn=(q@k.transpose(-2,-1))/self.sqrt_d\n",
    "        attn=F.softmax(attn,dim=-1)\n",
    "        attn_weight=self.dropout(attn)\n",
    "        # (B,heads,N,Dh) -> (B,N,heads,Dh) -> (B,N,D)\n",
    "        out=(attn_weight@v).transpose(1,2).reshape(B,N,D)\n",
    "        \n",
    "        if self.with_qkv:\n",
    "            out=self.w_o(out)\n",
    "        return out\n",
    "    \n",
    "class VitEncoderBlock(nn.Module):\n",
    "    def __init__(self,emb_dim:int=128,num_heads:int=2,mlp_ratio:int=2\n",
    "                 ,drop:float=0.,attn_drop:float=0.):\n",
    "        \"\"\"\n",
    "        mlp_ratio : nn.Linearでemb_dimの何倍に埋め込むか\n",
    "        attn_drop : Attn用のdropout率\n",
    "        drop      : Block用のdropout率\n",
    "        \"\"\"\n",
    "        super(VitEncoderBlock,self).__init__()\n",
    "        self.dropout=nn.Dropout(drop)\n",
    "        self.ln1=nn.LayerNorm(emb_dim)\n",
    "        self.mhsa=MHSA(emb_dim=emb_dim,num_heads=num_heads,dropout=attn_drop)\n",
    "        self.ln2=nn.LayerNorm(emb_dim)\n",
    "        self.mlp=nn.Sequential(\n",
    "            nn.Linear(emb_dim,emb_dim*mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(emb_dim*mlp_ratio,emb_dim),\n",
    "            nn.Dropout(drop))\n",
    "    \n",
    "    def forward(self,z:torch.Tensor)->torch.Tensor:\n",
    "        out1=self.mhsa(self.ln1(z))+z\n",
    "        out2=self.mlp(self.ln2(out1))+out1    \n",
    "        return out2\n",
    "\n",
    "class testModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                num_classes:int=10,\n",
    "                emb_dim:int=128,\n",
    "                num_blocks:int=7,\n",
    "                head_num:int=2,\n",
    "                kernel_size=(2,8,8),\n",
    "                mlp_ratio:int=3,\n",
    "                drop:float=0.,\n",
    "                attn_drop:float=0.):\n",
    "        super().__init__()\n",
    "        self.emb_layer=VitembLayer(kernel_size=kernel_size)\n",
    "        self.vit_encoder=nn.Sequential(\n",
    "            *[VitEncoderBlock(emb_dim,head_num,mlp_ratio,drop,attn_drop) \n",
    "             for _ in range(num_blocks)])\n",
    "        self.mlp_head=nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.Linear(emb_dim,num_classes))\n",
    "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
    "        input_x=self.emb_layer(x)\n",
    "        out=self.vit_encoder(input_x)\n",
    "        cls_token=out[:,0]\n",
    "        pred=self.mlp_head(cls_token)\n",
    "        return pred\n",
    "\n",
    "model=testModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a38d5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model(model,dataloaders_dict,criterion,optimizer,args):\n",
    "    start_time=time.time()\n",
    "    \n",
    "    train_acc_history=[]\n",
    "    val_acc_history=[]\n",
    "    train_loss_history=[]\n",
    "    val_loss_history=[]\n",
    "    train_f1_history=[]\n",
    "    val_f1_history=[]\n",
    "    plot_epoch=[]\n",
    "    \n",
    "    best_model_wts=copy.deepcopy(model.state_dict())\n",
    "    best_acc=0.0\n",
    "    best_epoch=0\n",
    "    model.to(device)\n",
    "    num_epoch=args.num_epochs\n",
    "    for epoch in range(1,num_epoch+1):\n",
    "        for phase in ['train','val']:\n",
    "            epoch_time=time.time()\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                train_pred_class=[]\n",
    "                train_ground_truths=[]\n",
    "            else:\n",
    "                model.eval()\n",
    "                val_pred_class=[]\n",
    "                val_ground_truths=[]\n",
    "            running_loss=0.0\n",
    "            running_corrects=0\n",
    "            train_n_total=1\n",
    "            \n",
    "            pbar=tqdm(dataloaders_dict[phase])\n",
    "            i=0\n",
    "            for sample in pbar:\n",
    "                #動画のみなら 'video'  joints なら 'joints' \n",
    "                inputs=sample['video'].to(device)\n",
    "                #inputs=sample['joints'].to(device)\n",
    "                \n",
    "                labels=sample['action'].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs=model(inputs)\n",
    "                    loss=criterion(outputs,torch.max(labels,1)[1])\n",
    "                    \n",
    "                    _,preds=torch.max(outputs,1)\n",
    "                    \n",
    "                    if phase=='train':\n",
    "                        train_pred_class.extend(preds.detach().cpu().numpy())\n",
    "                        train_ground_truths.extend(torch.max(labels,1)[1].detach().cpu().numpy())\n",
    "                    else:\n",
    "                        val_pred_class.extend(preds.detach().cpu().numpy())\n",
    "                        val_ground_truths.extend(torch.max(labels,1)[1].detach().cpu().numpy())\n",
    "                    if phase=='train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                running_loss+=loss.item()*inputs.size(0)\n",
    "                running_corrects+=torch.sum(preds==torch.max(labels,1)[1])\n",
    "                \n",
    "                pbar.set_description('Phase: {} || Epoch: {} || Loss{:.5f}'.format(phase,epoch,running_loss/train_n_total))\n",
    "                train_n_total+=1\n",
    "            \n",
    "            epoch_loss=running_loss/len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc=running_corrects/len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "            #Train and Val have done\n",
    "            epoch_end_time=time.time()-epoch_time\n",
    "            if phase=='train':\n",
    "                print('Training Complete in {:.0f}m {:.0f}s '.format(epoch_end_time//60,epoch_end_time%60))\n",
    "            else:\n",
    "                print('Validation Complete in {:.0f}m {:.0f}s '.format(epoch_end_time//60,epoch_end_time%60))\n",
    "            #print('{} Loss: {:.4f} Acc:{:.4f} % '.format(phase,epoch_loss,epoch_acc))\n",
    "            \n",
    "            if phase =='train':\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                train_loss_history.append(epoch_loss)\n",
    "                train_pred_classes=np.asarray(train_pred_class)\n",
    "                train_ground_truths=np.asarray(train_ground_truths)\n",
    "                \n",
    "                train_accuracy,train_precision,train_recall,train_f1=utils.Get_scores(\n",
    "                    train_pred_classes , train_ground_truths)\n",
    "                \n",
    "                train_f1_history.append(train_f1)\n",
    "                train_confusion_matrix=np.array_str(confusion_matrix(train_ground_truths,train_pred_class,labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "                print('Epoch: {} || Train_Acc: {} || Train_Loss: {}'.format(\n",
    "                    epoch, train_accuracy, epoch_loss\n",
    "                ))\n",
    "                print(f'train: \\n{train_confusion_matrix}')\n",
    "                plot_epoch.append(epoch)\n",
    "                \n",
    "                train_loss=epoch_loss\n",
    "            \n",
    "            # For Checkpointing and Confusion Matrix\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                val_pred_classes = np.asarray(val_pred_class)\n",
    "                val_ground_truths = np.asarray(val_ground_truths)\n",
    "                \n",
    "                val_accuracy,val_precision,val_recall,val_f1 = utils.Get_scores(\n",
    "                    val_pred_classes, val_ground_truths\n",
    "                )\n",
    "                \n",
    "                val_f1_history.append(val_f1)\n",
    "                val_confusion_matrix = np.array_str(confusion_matrix(val_ground_truths, val_pred_classes, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "                print('Epoch: {} || Val_Acc: {} || Val_Loss: {}'.format(\n",
    "                    epoch, val_accuracy, epoch_loss\n",
    "                ))\n",
    "                print(f'val: \\n{val_confusion_matrix}')\n",
    "\n",
    "                # Deep Copy Model if best accuracy\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    best_epoch=epoch+1\n",
    "                    \n",
    "                # set current loss to val loss for write history\n",
    "                val_loss = epoch_loss\n",
    "            \n",
    "            \n",
    "        #モデル/optimizerを保存\n",
    "        model_name= utils.save_weights(model, args, epoch, optimizer)\n",
    "            \n",
    "        #writehistory\n",
    "        # Write History after train and validation phase\n",
    "        utils.write_history(\n",
    "                    args,\n",
    "                    epoch,\n",
    "                    train_loss,train_accuracy,train_f1,train_precision,train_recall,train_confusion_matrix,\n",
    "                    val_loss,val_accuracy,val_f1,val_precision,val_recall,val_confusion_matrix,\n",
    "                    best_acc)\n",
    "        \n",
    "        \n",
    "    end=time.time()-start_time\n",
    "    print('all done in {:.0f}m {:.0f}s'.format(end//60,end%60))\n",
    "    print('Best val Acc {:.4f}'.format(best_acc))\n",
    "    history_path=args.history_path+args.base_model+'.txt'\n",
    "    with open(history_path,'a') as file:\n",
    "        f.write('All Done in {:.0f}m {:.0f}s'.format(end//60,end%60))\n",
    "    #load best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    save_path=os.path.join(args.model_path,'best_weights_{}.pth'.format(best_epoch))\n",
    "    torch.save(model.state_dict(),save_path)\n",
    "    return model, train_loss_history, val_loss_history, train_acc_history, val_acc_history, train_f1, val_f1, plot_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce4d567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=Dataset.BasketballDataset(annotation_dict='../transformermodels/dataset/annotation_dict.json',\n",
    "                                    augmented_dict='../transformermodels/dataset/augmented_annotation_dict.json',\n",
    "                                    video_dir='../transformermodels/dataset/examples/',\n",
    "                                    augmented_video_dir='../transformermodels/dataset/augmented-examples/',\n",
    "                                    augment=True,poseData=False,joints_to_numpy=False)\n",
    "\n",
    "\n",
    "train_dataset_size=dataset.__len__()\n",
    "train_num=int(train_dataset_size*0.7)\n",
    "val_num=train_dataset_size-train_num\n",
    "### ver 2.\n",
    "train_dataset,val_dataset=torch.utils.data.random_split(dataset,[train_num,val_num])\n",
    "#test用のsubset\n",
    "#train_subset=Subset(train_dataset,list(range(0,70)))\n",
    "#val_subset=Subset(val_dataset,list(range(0,30)))\n",
    "\n",
    "train_loader=DataLoader(train_dataset,shuffle=True,batch_size=4)\n",
    "val_loader=DataLoader(val_dataset,shuffle=False,batch_size=4)\n",
    "dataloaders_dict={'train':train_loader,'val':val_loader}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ad631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase: train || Epoch: 1 || Loss9.66084:   2%|█▋                                                                        | 205/8733 [00:28<20:00,  7.11it/s]"
     ]
    }
   ],
   "source": [
    "model=testModel()\n",
    "optimizer=optim.Adam(model.parameters(),lr=args.lr)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model, train_loss_history, val_loss_history,train_acc_history, val_acc_history, train_f1_score, val_f1_score, plot_epoch = Train_model(model,dataloaders_dict,criterion,optimizer,args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd27f8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
