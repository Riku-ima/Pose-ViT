{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0440d65",
   "metadata": {},
   "source": [
    "## Timesformer + AcT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccaec547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from collections import OrderedDict\n",
    "from torchinfo import summary\n",
    "#import Dataset\n",
    "from torch.utils.data import Subset,Dataset\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "\n",
    "from einops import rearrange,reduce,repeat\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "359fe0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasketballDataset(Dataset):\n",
    "    \"\"\"SpaceJam: a Dataset for Basketball Action Recognition.\n",
    "    \n",
    "    data_dir: normal and augment\n",
    "    anno_dir: noraml and augment \n",
    "    \n",
    "    \n",
    "    Return------------\n",
    "    sample : dictionary{  video:torch.tensor(action movie) , action: action label one-hot ,class :labels  \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, annotation_dict, augmented_dict, video_dir=\"./dataset/examples/\", \n",
    "                 augmented_video_dir=\"./dataset/augmented-examples/\", augment=True, transform=None, poseData=False,joints_to_numpy=False):\n",
    "        with open(annotation_dict) as f:\n",
    "            self.anno_list = list(json.load(f).items())\n",
    "        \n",
    "        if augment==True:\n",
    "            self.augment=augment\n",
    "            with open(augmented_dict)  as f:\n",
    "                augment_anno_list=list(json.load(f).items())\n",
    "            self.augmented_video_dir=augmented_video_dir\n",
    "            self.anno_list.extend(augment_anno_list)\n",
    "            \n",
    "        self.video_dir=video_dir\n",
    "        self.poseData=poseData\n",
    "        self.transform=transform\n",
    "        self.augment=augment\n",
    "        self.joints_to_numpy=joints_to_numpy\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.anno_list))\n",
    "    def keystoint(self,x):\n",
    "        return {int(k): v for k,v in x.items()}\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        video_id=self.anno_list[idx][0]\n",
    "        #one-hot vector\n",
    "        encoding=np.squeeze(np.eye(10)[np.array([0,1,2,3,4,5,6,7,8,9]).reshape(-1)])\n",
    "        \n",
    "        joints=np.load(self.video_dir+video_id+'.npy',allow_pickle=True)\n",
    "        Video=self.VideoToNumpy(video_id)\n",
    "        joints=self.joint2numpy(joints=joints)\n",
    "        sample={'video_id':video_id,'video':torch.from_numpy(Video).float(),'joints':torch.from_numpy(joints) , 'action':torch.from_numpy(\n",
    "            np.array(encoding[self.anno_list[idx][1]])),'class':self.anno_list[idx][1]}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def VideoToNumpy(self,video_id,sampling_rate=1):\n",
    "        #まず拡張していない動画探索 get video from normal video dir\n",
    "        video_path=self.video_dir+video_id+'.mp4'\n",
    "        Video=cv2.VideoCapture(video_path)\n",
    "        #FRAMES=Video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        if not Video.isOpened():\n",
    "            #拡張したdirから探索  from augment video dir\n",
    "            video_path=self.augmented_video_dir+video_id+'.mp4'\n",
    "            Video=cv2.VideoCapture(video_path)\n",
    "        if not Video.isOpened():\n",
    "            raise Exception('Video file not exist or readable!!')\n",
    "        \n",
    "        video_frames=[]\n",
    "        \n",
    "        while (Video.isOpened):\n",
    "            frame_num=1\n",
    "            ret,frame=Video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if  frame_num==1 or (frame_num-1)%sampling_rate==0:\n",
    "                #print(frame)\n",
    "                out_frame=np.asarray([frame[..., i] for i in range(frame.shape[-1])]).astype(float)\n",
    "                #new_frame_3=frame.transpose(2,0,1).astype(float)\n",
    "                video_frames.append(out_frame)\n",
    "            frame_num+=1\n",
    "        Video.release()\n",
    "        assert len(video_frames)==16\n",
    "        #return \n",
    "        #　動画から16frame 抽出  (c,f,h,w)\n",
    "        return np.transpose(np.asarray(video_frames),(1,0,2,3))\n",
    "    \n",
    "    def joint2numpy(self,joints):\n",
    "        \"\"\"\n",
    "        joints: numpy dictionary    16 frames × 18 joints\n",
    "        \"\"\"\n",
    "        joints_arr=np.zeros([len(joints),18*2],dtype=np.float32)\n",
    "        for idx in range(len(joints)):\n",
    "            frame=joints[idx]\n",
    "            arr=np.array(list(frame.values()))\n",
    "            #検出してないjointの座標を(0,0)におく\n",
    "            for i in range(18):\n",
    "                if i not in frame.keys():\n",
    "                    arr=np.insert(arr,i,0,axis=0)\n",
    "            arr=arr.flatten()\n",
    "            joints_arr[idx]=arr\n",
    "\n",
    "        return joints_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "755f5dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from collections import OrderedDict\n",
    "from torchinfo import summary\n",
    "#import Dataset\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from einops import rearrange,reduce,repeat\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, container_abcs.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "    return parse\n",
    "to_2tuple = _ntuple(2)\n",
    "\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self,emb_dim:int=128,head:int=2,dropout:float=0.,with_qkv=True):\n",
    "        \"\"\"\n",
    "        with_qkv :\n",
    "        -> True なら、MHSAのために、3倍のdimに入力をq,k,vに埋め込む\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MHSA,self).__init__()\n",
    "        self.num_heads=head\n",
    "        self.emb_dim=emb_dim\n",
    "        self.head_dim=emb_dim//self.num_heads\n",
    "        self.sqrt_d=emb_dim**0.5\n",
    "        self.with_qkv=with_qkv\n",
    "        \n",
    "        #埋め込み\n",
    "        if with_qkv:\n",
    "            self.w_qkv=nn.Linear(self.emb_dim,self.emb_dim*3,bias=False)\n",
    "            self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "        self.w_o=nn.Sequential(\n",
    "            nn.Linear(emb_dim,emb_dim),\n",
    "            nn.Dropout(dropout))\n",
    "        \n",
    "    def forward(self,z:torch.Tensor)->torch.Tensor:\n",
    "        B,N,D=z.shape\n",
    "        if self.with_qkv:\n",
    "            #->(B,N,3,num_heads,head_dim)\n",
    "        \n",
    "            z=self.w_qkv(z).reshape(B,N,3,self.num_heads,self.head_dim)\n",
    "            # ->(3,B,num_heads,N,head_dim)\n",
    "            z=z.permute(2,0,3,1,4)\n",
    "            q,k,v=z[0],z[1],z[2]\n",
    "        #埋め込みが不要なら、ただqkv複製\n",
    "        else:\n",
    "            z=z.reshape(B,N,self.num_heads,self.head_dim)\n",
    "            #->(B.num_heads ,N ,head_dim )\n",
    "            z=z.permute(0,2,1,3)\n",
    "            q,k,v=z,z,z\n",
    "        \n",
    "        attn=(q@k.transpose(-2,-1))/self.sqrt_d\n",
    "        attn=F.softmax(attn,dim=-1)\n",
    "        attn_weight=self.dropout(attn)\n",
    "        # (B,heads,N,Dh) -> (B,N,heads,Dh) -> (B,N,D)\n",
    "        out=(attn_weight@v).transpose(1,2).reshape(B,N,D)\n",
    "        \n",
    "        if self.with_qkv:\n",
    "            out=self.w_o(out)\n",
    "        return out\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self,emb_dim:int=384,num_heads:int=2,mlp_ratio=4,\n",
    "                 drop:float=0.,attn_drop:float=0.,drop_path=0.1,\n",
    "                attention_type='divided_space_time'):\n",
    "        \n",
    "        \"\"\"\n",
    "        mlp_ratio : nn.Linearでemb_dimの何倍に埋め込むか\n",
    "        attn_drop : Attn用のdropout率\n",
    "        drop      : Block用のdropout率\n",
    "        \"\"\"\n",
    "        super(Block,self).__init__()\n",
    "        self.attention_type=attention_type\n",
    "        assert(attention_type in ['divided_space_time', 'space_only','joint_space_time'])\n",
    "        \n",
    "        #spational 用 Attn \n",
    "        self.drop_path=DropPath(drop_path) if drop_path>0. else nn.Identity()\n",
    "        self.norm_layer=nn.LayerNorm(emb_dim)\n",
    "        self.attn=MHSA(emb_dim=emb_dim,head=num_heads,dropout=attn_drop)\n",
    "        \n",
    "        #temporal\n",
    "        if self.attention_type=='divided_space_time':\n",
    "            self.temporal_norm=nn.LayerNorm(emb_dim)\n",
    "            self.temporal_attn=MHSA(emb_dim=emb_dim,head=num_heads,dropout=attn_drop)\n",
    "            self.temporal_fc=nn.Linear(emb_dim,emb_dim)\n",
    "    \n",
    "        self.mlp=nn.Sequential(\n",
    "            nn.Linear(emb_dim,int(emb_dim*mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(int(emb_dim*mlp_ratio),emb_dim),\n",
    "            nn.Dropout(drop)\n",
    "            )\n",
    "    \n",
    "    def forward(self,z,T:int=16):\n",
    "        \"\"\"\n",
    "        input : z -> (B,N,D)  ,T frames ,P keypoints\n",
    "        --------------------------------------------------\n",
    "        B: batch_num , \n",
    "        N (H*W*frame)+1   今回は (keypoints × T frames )+1\n",
    "        D;emb_dim  \n",
    "        \"\"\"\n",
    "        B=z.size(0)\n",
    "        P=z.size(1)//T\n",
    "        if self.attention_type in ['space_only','joint_space_time']:\n",
    "            z=z+self.drop_path(self.attn(self.norm_layer(z)))\n",
    "            z=z+self.drop_path(self.mlp(self.norm_layer(z)))\n",
    "            return z\n",
    "        \n",
    "        elif self.attention_type=='divided_space_time':\n",
    "            ###  Temporal  ###\n",
    "            #同一のパッチの箇所で、時間でattn  ,cls_tokenは処理しない!!!\n",
    "            #zt (B,t*h*w,d)-> (B*h*w,t,d)\n",
    "            zt=z[:,1:,:]\n",
    "            zt = rearrange(zt, 'b (p t) d -> (b p) t d',b=B,p=P,t=T)\n",
    "            res_temporal=self.drop_path(self.temporal_attn(self.temporal_norm(zt)))\n",
    "            #(B*P,T,D) -> (B,P*t,d)\n",
    "            res_temporal=rearrange(res_temporal,'(b p) t d -> b (p t) d',b=B,p=P,t=T)\n",
    "            \n",
    "            res_temporal=self.temporal_fc(res_temporal)\n",
    "            #cls_token以外に接続\n",
    "            zt=z[:,1:,:]+res_temporal\n",
    "            ###  Spatial  ###\n",
    "            init_cls_token =z[:,0,:].unsqueeze(1)\n",
    "            #各フレーム分クラストークンを複製\n",
    "            cls_token=init_cls_token.repeat(1,T,1)\n",
    "            cls_token = rearrange(cls_token, 'b t m -> (b t) m',b=B,t=T).unsqueeze(1)\n",
    "            zs=zt\n",
    "            zs=rearrange(zs,'b (p t) m -> (b t) p m',b=B,p=P,t=T)\n",
    "            zs=torch.cat((cls_token,zs),1)\n",
    "            res_spatial=self.drop_path(self.attn(self.norm_layer(zs)))     \n",
    "            \n",
    "            cls_token=res_spatial[:,0,:]\n",
    "            cls_token=rearrange(cls_token,'(b t) m -> b t m',b=B,t=T)\n",
    "            #cls_tokenについて各frameの平均をとる\n",
    "            cls_token=torch.mean(cls_token,1,True)\n",
    "            \n",
    "            res_spatial=res_spatial[:,1:,:]\n",
    "            res_spatial=rearrange(res_spatial,'(b t) p m -> b (t p) m',b=B,t=T,p=P)\n",
    "\n",
    "            res=res_spatial\n",
    "            z=zt\n",
    "            \n",
    "            #  temporal_out(init_cls_token) + temporal_spatial_out(cls_token)\n",
    "            z=torch.cat((init_cls_token,z),1)+torch.cat((cls_token,res),1)\n",
    "            z=z+self.drop_path(self.mlp(self.norm_layer(z)))\n",
    "            return z\n",
    "        \n",
    "class PatchEmbLayer(nn.Module):\n",
    "    def __init__(self,in_features:int=2,emb_dim:int=8,t:int=16,p:int=18):\n",
    "        \"\"\"\n",
    "        in_features : 各keypoints座標2次元\n",
    "        emb_dim : 各keypoint(x,y)を何次元に埋め込むか、要検討\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.emb_dim=emb_dim\n",
    "        self.emb_layer=nn.Linear(in_features,emb_dim)\n",
    "        \n",
    "        self.num_patch=t*p  #T*P\n",
    "        self.cls_token=nn.Parameter(torch.randn(1,1,emb_dim))\n",
    "        self.pos_emb=nn.Parameter(\n",
    "            torch.randn(1,self.num_patch+1,emb_dim)\n",
    "            )\n",
    "    def forward(self,z) ->torch.Tensor:\n",
    "        \"\"\"\n",
    "        入力z || (B,T,P) B:batch , T :frames , P :keypoints\n",
    "        \"\"\"\n",
    "        B,T,P=z.shape[0],z.shape[1],z.shape[2]\n",
    "        z=rearrange(z,'b t (k d) -> b t k d',b=B,t=T,d=2)\n",
    "        #->b t p emb_dim\n",
    "        z=self.emb_layer(z)\n",
    "        z=rearrange(z,'b t p e -> b (t p) e',b=B,t=T,p=P//2)\n",
    "        cls_token=self.cls_token.repeat(repeats=(z.size(0),1,1))\n",
    "        z_0=torch.cat((cls_token,z),1)\n",
    "        z_0=z_0+self.pos_emb\n",
    "        return z_0\n",
    "\n",
    "class Times_AcT(nn.Module):\n",
    "    def __init__(self,num_classes:int=10,emb_dim:int=8,num_blocks:int=7,\n",
    "                 head_num:int=2,mlp_ratio:int=4,drop:float=0.,attn_drop:float=0.):\n",
    "        super().__init__()\n",
    "        self.input_emb_layer=PatchEmbLayer(emb_dim=emb_dim)\n",
    "        self.vit_encoder=nn.Sequential(\n",
    "            *[Block(emb_dim=emb_dim,num_heads=head_num,mlp_ratio=mlp_ratio,drop=drop,attn_drop=attn_drop)\n",
    "             for _ in range(num_blocks)] )\n",
    "        \n",
    "        self.mlp_head=nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.Linear(emb_dim,num_classes))\n",
    "    \n",
    "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
    "        input_x=self.input_emb_layer(x)\n",
    "        print(input_x.shape)\n",
    "        out=self.vit_encoder(input_x)\n",
    "        cls_token=out[:,0]\n",
    "        pred=self.mlp_head(cls_token)\n",
    "        return pred\n",
    "    \n",
    "def generate_model(emb_dim:int=128):\n",
    "    \n",
    "    model=Times_AcT(emb_dim=128)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0a6d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=BasketballDataset(annotation_dict='./dataset/annotation_dict.json',augmented_dict='./dataset/augmented_annotation_dict.json')\n",
    "inputs=dataset.__getitem__(19)\n",
    "inputs=inputs['joints'].unsqueeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c87dc603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 289, 128])\n"
     ]
    }
   ],
   "source": [
    "model=generate_model()\n",
    "out=model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6245550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
